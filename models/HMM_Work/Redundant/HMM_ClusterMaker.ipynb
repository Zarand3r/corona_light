{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import simplejson\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, leaves_list\n",
    "from matplotlib import pyplot as plt\n",
    "import git\n",
    "import sys\n",
    "repo = git.Repo(\"./\", search_parent_directories=True)\n",
    "homedir = repo.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHMMUnSupData(Input, colname, fipsname):\n",
    "    #Takes input dataframe, and gives out HMM format of Input data, a list of lists \n",
    "    #of the colname value, each list in the set represents one fips code.\n",
    "    Output = []\n",
    "    for fips in Input[fipsname].unique():\n",
    "        temp = list(Input[Input[fipsname] == fips][colname])\n",
    "        Output.append(temp)\n",
    "    return Output\n",
    "\n",
    "def monotonicCol(Data, colname):\n",
    "    #Takes a column that should have monotonically increasing data for a column (number of deaths)\n",
    "    #and adjusts the column to ensure this property, iterating backwards through each fips code's entries\n",
    "    ls = []\n",
    "    tempvals = []\n",
    "    for fips in Data.FIPS.unique():\n",
    "        vals = list(Data[Data['FIPS'] == fips][colname])\n",
    "        flag = True\n",
    "        for val in reversed(vals):\n",
    "            if flag:\n",
    "                flag = False\n",
    "                maxval = val\n",
    "                tempvals.append(maxval)\n",
    "            else:\n",
    "                if val > maxval:\n",
    "                    tempvals.append(maxval)\n",
    "                else:\n",
    "                    maxval = val\n",
    "                    tempvals.append(val)\n",
    "        ls.extend(reversed(tempvals))\n",
    "        tempvals = []\n",
    "    return ls\n",
    "\n",
    "def cumtoDaily(Data, colname):\n",
    "    #Takes cumulative column data and turns the data into daily changes \n",
    "    ls = []\n",
    "    column = Data[colname]\n",
    "    for fips in Data.FIPS.unique():\n",
    "        ls.extend(list(Data[Data['FIPS'] == fips][colname].diff().fillna(0)))\n",
    "    return ls\n",
    "\n",
    "def znormalize(ls):\n",
    "#normalizes a list, if std=0 return the list\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        return np.array(ls)\n",
    "    else:\n",
    "        val = (ls - np.mean(ls))/np.std(ls)\n",
    "        return (ls - np.mean(ls))/np.std(ls)\n",
    "\n",
    "def znormalize_nozeros(ls):\n",
    "#normalizes a list, if std=0 just pass\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        pass\n",
    "    else:\n",
    "        return (ls - np.mean(ls))/np.std(ls)\n",
    "    \n",
    "def noinf(arr):\n",
    "    #Removes inf from list of lists\n",
    "    newarr = []\n",
    "    for x in arr:\n",
    "        temp = x\n",
    "        temp[temp == np.inf] = 9999\n",
    "        newarr.append(x)\n",
    "    return newarr\n",
    "\n",
    "def nonzerofips(arr):\n",
    "    #Takes in dataset, returns indices of data that do not have a list with all 0's\n",
    "    ind = []\n",
    "    for i in range(len(arr)):\n",
    "        if np.std(arr[i]) != 0:\n",
    "            ind.append(i)\n",
    "    return ind\n",
    "\n",
    "def makeZ(Data):\n",
    "    #Creates DTW linkage matrix using DTAIdistance and scipy\n",
    "    distance = dtw.distance_matrix_fast(Data,compact=True)\n",
    "    Z = linkage(distance, method='complete')\n",
    "    return Z\n",
    "\n",
    "def fillnonzero(OrigData, clusters):\n",
    "    #Takes a clustering from a dataset with nonzero entries.\n",
    "    #Adds to that clustering another cluster for all 0's\n",
    "    n = 0\n",
    "    newclusters = []\n",
    "    for i in range(len(OrigData)):\n",
    "        if np.std(OrigData[i]) == 0:\n",
    "            newclusters.append(0)\n",
    "        else:\n",
    "            newclusters.append(clusters[n])\n",
    "            n += 1\n",
    "    return newclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #NYT Data (NYT_W and NYT_F)\n",
    "    #Differenced Daily Death Data\n",
    "    NYT_daily = pd.read_csv(f\"{homedir}/data/us/covid/nyt_us_counties_daily.csv\")\n",
    "    NYT_daily = NYT_daily.drop(columns=['county','state']).sort_values(['fips','date']).reset_index(drop=True)\n",
    "    NYT_daily['fips'] = NYT_daily.fips.astype(int)\n",
    "    NYT_daily['date'] = pd.to_datetime(NYT_daily['date'])\n",
    "    NYT_daily['id'] = NYT_daily.fips.astype(str).str.cat(NYT_daily.date.astype(str), sep=', ')\n",
    "    FirstDay = min(NYT_daily.date.unique())\n",
    "    LastDay = max(NYT_daily.date.unique())\n",
    "\n",
    "    #Making a time-warping of NYT daily data, so each county has a value at the starting day of 2020-01-21, the second value is\n",
    "    #the date of the first reported date from NYT\n",
    "    # and then a final value at the most recent day\n",
    "    NYT_daily_Warp = NYT_daily\n",
    "    for fips in NYT_daily.fips.unique():\n",
    "        rows = NYT_daily[NYT_daily['fips'] == fips]\n",
    "        #adding in the first day values\n",
    "        if FirstDay not in rows.date.unique():\n",
    "            NYT_daily_Warp = NYT_daily_Warp.append({'fips': fips, 'date': pd.to_datetime('2020-01-21'), \\\n",
    "                                   'cases': 0, 'deaths' : 0, 'id' : str(fips) + ', 2020-01-21'}, ignore_index=True)\n",
    "        #making sure each entry has the final day values\n",
    "        if LastDay not in rows.date.unique():\n",
    "            NYT_daily_Warp = NYT_daily_Warp[NYT_daily_Warp['fips'] != fips]\n",
    "    NYT_daily_Warp = NYT_daily_Warp.sort_values(['fips','date']).reset_index(drop=True)\n",
    "    NYT_daily_Warp.to_csv('NYT_daily_Warp.csv')\n",
    "    NYT_daily_Warp_Death = makeHMMUnSupData(NYT_daily_Warp, 'deaths', 'fips')\n",
    "\n",
    "    #This is a list of all the counties and dates\n",
    "    County_List = list(NYT_daily.fips.unique())\n",
    "    Date_List = list(NYT_daily.date.unique())\n",
    "    #This creates a base dataframe that contains all pairs of FIPS codes with the valid dates given in Air_Qual\n",
    "    CL, DL = pd.core.reshape.util.cartesian_product([County_List, Date_List])\n",
    "    BaseFrame = pd.DataFrame(dict(fips=CL, date=DL)).sort_values(['fips','date']).reset_index(drop=True)\n",
    "    BaseFrame['id'] = BaseFrame.fips.astype(str).str.cat(BaseFrame.date.astype(str), sep=', ')\n",
    "\n",
    "    #Making frame of all deaths at all dates to properly do DTW clustering\n",
    "    NYT_daily_Filled = BaseFrame.join(NYT_daily.set_index('id'), on='id', how='outer', lsuffix='',rsuffix='_x').sort_values(['fips', 'date']).drop(columns=['fips_x','date_x']).fillna(0).drop_duplicates(subset=['fips','date']).reset_index(drop=True)\n",
    "    NYT_daily_Filled.to_csv('NYT_daily_Filled.csv')\n",
    "    #List of lists of daily death count for each county, starting 1/23/20, ending most recent date.\n",
    "    NYT_daily_Death_Filled = makeHMMUnSupData(NYT_daily_Filled, 'deaths', 'fips')\n",
    "    #JHU Data\n",
    "    JHU_tot = pd.read_csv(f\"{homedir}/data/us/covid/JHU_daily_US.csv\").sort_values(['FIPS','Date'])\n",
    "    FIPSlist = JHU_tot.FIPS.unique()\n",
    "    Datelist = JHU_tot.Date.unique()\n",
    "    Datepair = [Datelist[0],Datelist[-1]]\n",
    "\n",
    "    #Getting rid of unneded fips code in the list of total codes\n",
    "    for fips in FIPSlist:\n",
    "        rows = JHU_tot[JHU_tot['FIPS'] == fips]\n",
    "        datelist = rows.Date.unique()\n",
    "        datepair = [datelist[0],datelist[-1]]\n",
    "        if np.array_equal(Datepair,datepair) != True:\n",
    "            JHU_tot = JHU_tot.drop(list(JHU_tot[JHU_tot['FIPS'] == fips].index))\n",
    "    JHU_tot = JHU_tot.sort_values(['FIPS','Date']).reset_index(drop=True)\n",
    "\n",
    "    d = {'FIPS': JHU_tot['FIPS'], 'Date' : JHU_tot['Date'], 'Confirmed' : monotonicCol(JHU_tot,'Confirmed'),\\\n",
    "           'Deaths' : monotonicCol(JHU_tot,'Deaths'),'Active' : monotonicCol(JHU_tot,'Active'), \\\n",
    "            'Recovered' : monotonicCol(JHU_tot,'Recovered')}\n",
    "    #Monotonically increaasing transformation of JHU_tot\n",
    "    JHU_mono = pd.DataFrame(data=d)\n",
    "\n",
    "    d = {'FIPS': JHU_mono['FIPS'], 'Date' : JHU_mono['Date'], 'Confirmed' : cumtoDaily(JHU_mono,'Confirmed'),\\\n",
    "           'Deaths' : cumtoDaily(JHU_mono,'Deaths'),'Active' : cumtoDaily(JHU_mono,'Active'), \\\n",
    "            'Recovered' : cumtoDaily(JHU_mono,'Recovered')}\n",
    "    #Daily changing data based on monotonically transformed data\n",
    "    JHU_daily = pd.DataFrame(data=d)\n",
    "    JHU_daily.to_csv('JHU_Daily.csv')\n",
    "    #List of lists of daily death count for each county, starting 3/23/20, ending most recent date.\n",
    "    JHU_daily_death = makeHMMUnSupData(JHU_daily, 'Deaths', 'FIPS')\n",
    "    \n",
    "    NYT_F = NYT_daily_Filled\n",
    "    NYT_W = NYT_daily_Warp\n",
    "    JHU = JHU_daily\n",
    "    \n",
    "    #Original dataset, making into list of np arrays\n",
    "    NYT_daily_Warp_Death = [np.array(x) for x in NYT_daily_Warp_Death]\n",
    "    NYT_daily_Death_Filled = [np.array(x) for x in NYT_daily_Death_Filled]\n",
    "    JHU_daily_death = [np.array(x) for x in JHU_daily_death]\n",
    "    \n",
    "    #Saving the death data files\n",
    "    f = open('NYT_daily_Warp_Death.txt', 'w')\n",
    "    simplejson.dump(NYT_daily_Warp_Death, f)\n",
    "    f.close()\n",
    "    g = open('NYT_daily_Death_Filled.txt', 'w')\n",
    "    simplejson.dump(NYT_daily_Death_Filled, g)\n",
    "    g.close()\n",
    "    h = open('JHU_daily_death.txt', 'w')\n",
    "    simplejson.dump(JHU_daily_death, h)\n",
    "    h.close()\n",
    "\n",
    "    #Z normalization of our dataset\n",
    "    Series_NYT_W = [znormalize(x) for x in NYT_daily_Warp_Death]\n",
    "    Series_NYT_F = [znormalize(x) for x in NYT_daily_Death_Filled]\n",
    "    Series_JHU = [znormalize(x) for x in JHU_daily_death]\n",
    "\n",
    "    #Removal of Strictly 0 lists from our dataset, these will belong in cluster 0\n",
    "    Series_NYT_W_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Warp_Death]\n",
    "    Series_NYT_W_nozeros =  [x for x in Series_NYT_W_nozeros if x is not None]\n",
    "\n",
    "    Series_NYT_F_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Death_Filled]\n",
    "    Series_NYT_F_nozeros =  [x for x in Series_NYT_F_nozeros if x is not None]\n",
    "\n",
    "    Series_JHU_nozeros = [znormalize_nozeros(x) for x in JHU_daily_death]\n",
    "    Series_JHU_nozeros =  [x for x in Series_JHU_nozeros if x is not None]\n",
    "    \n",
    "    #We generate the many clusters needed for analysis\n",
    "    #Suffix \"O\": uses original unedited data\n",
    "    #\"Z\": uses z-normalized data, \"N\": uses z-normalized data, with all 0's entries in individual cluster\n",
    "    #\"T\": represents Tight, means a lower nubmer of clusters used\n",
    "    #\"L\": represents Loose, a higher number of clusters used\n",
    "    JHU_Cluster_Size = [2,3,6,3,6]\n",
    "\n",
    "    Z_JHU_O = makeZ(JHU_daily_death)\n",
    "    Z_JHU_Z = makeZ(Series_JHU)\n",
    "    Z_JHU_N = makeZ(Series_JHU_nozeros)\n",
    "\n",
    "    JHU_O = fcluster(Z_JHU_O, JHU_Cluster_Size[0], criterion ='maxclust')\n",
    "    JHU_Z_T = fcluster(Z_JHU_Z, JHU_Cluster_Size[1], criterion ='maxclust')\n",
    "    JHU_Z_L = fcluster(Z_JHU_Z, JHU_Cluster_Size[2], criterion ='maxclust')\n",
    "    JHU_N_T = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[3], criterion ='maxclust'))\n",
    "    JHU_N_L = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "    ClustersJHU = pd.DataFrame(data=JHU.FIPS.unique(),columns=['FIPS'])\n",
    "    ClustersJHU['JHU_Orig'] = JHU_O\n",
    "    ClustersJHU['JHU_Z_T'] = JHU_Z_T\n",
    "    ClustersJHU['JHU_Z_L'] = JHU_Z_L\n",
    "    ClustersJHU['JHU_N_T'] = JHU_N_T\n",
    "    ClustersJHU['JHU_N_L'] = JHU_N_L\n",
    "\n",
    "    NYT_F_Cluster_Size = [2,5,5]\n",
    "\n",
    "    Z_NYT_F_O = makeZ(NYT_daily_Death_Filled)\n",
    "    Z_NYT_F_Z = makeZ(Series_NYT_F)\n",
    "    Z_NYT_F_N = makeZ(Series_NYT_F_nozeros)\n",
    "\n",
    "    NYT_F_O = fcluster(Z_NYT_F_O, NYT_F_Cluster_Size[0], criterion ='maxclust')\n",
    "    NYT_F_Z = fcluster(Z_NYT_F_Z, NYT_F_Cluster_Size[1], criterion ='maxclust')\n",
    "    NYT_F_N = fillnonzero(Series_NYT_F,fcluster(Z_NYT_F_N, NYT_F_Cluster_Size[2], criterion ='maxclust'))\n",
    "\n",
    "    ClustersNYT_F = pd.DataFrame(data=NYT_F.fips.unique(),columns=['FIPS'])\n",
    "    ClustersNYT_F['NYT_F_Orig'] = NYT_F_O\n",
    "    ClustersNYT_F['NYT_F_Z'] = NYT_F_Z\n",
    "    ClustersNYT_F['NYT_F_N'] = NYT_F_N\n",
    "\n",
    "    NYT_W_Cluster_Size = [2,5,8,5,7]\n",
    "\n",
    "    Z_NYT_W_O = makeZ(NYT_daily_Warp_Death)\n",
    "    Z_NYT_W_Z = makeZ(Series_NYT_W)\n",
    "    Z_NYT_W_N = makeZ(Series_NYT_W_nozeros)\n",
    "\n",
    "    NYT_W_O = fcluster(Z_NYT_W_O, NYT_W_Cluster_Size[0], criterion ='maxclust')\n",
    "    NYT_W_Z_T = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[1], criterion ='maxclust')\n",
    "    NYT_W_Z_L = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[2], criterion ='maxclust')\n",
    "    NYT_W_N_T = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[3], criterion ='maxclust'))\n",
    "    NYT_W_N_L = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "    ClustersNYT_W = pd.DataFrame(data=NYT_W.fips.unique(),columns=['FIPS'])\n",
    "    ClustersNYT_W['NYT_W_Orig'] = NYT_W_O\n",
    "    ClustersNYT_W['NYT_W_Z_T'] = NYT_W_Z_T\n",
    "    ClustersNYT_W['NYT_W_Z_L'] = NYT_W_Z_L\n",
    "    ClustersNYT_W['NYT_W_N_T'] = NYT_W_N_T\n",
    "    ClustersNYT_W['NYT_W_N_L'] = NYT_W_N_L\n",
    "    \n",
    "    #Saving all the clusters in one dataframe\n",
    "    AllClusters = ClustersJHU.join(ClustersNYT_F.set_index('FIPS'), on='FIPS', \\\n",
    "                                   how='outer').join(ClustersNYT_W.set_index('FIPS'), on='FIPS', how='outer').sort_values('FIPS')\n",
    "    AllClusters.to_csv('DTW_Clustering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
