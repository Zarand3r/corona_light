{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import simplejson\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, leaves_list\n",
    "from hmmlearn import hmm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "import git\n",
    "import sys\n",
    "repo = git.Repo(\"./\", search_parent_directories=True)\n",
    "homedir = repo.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHMMUnSupData(Input, colname, fipsname):\n",
    "    #Takes input dataframe, and gives out HMM format of Input data, a list of lists \n",
    "    #of the colname value, each list in the set represents one fips code.\n",
    "    Output = []\n",
    "    for fips in Input[fipsname].unique():\n",
    "        temp = list(Input[Input[fipsname] == fips][colname])\n",
    "        Output.append(temp)\n",
    "    return Output\n",
    "\n",
    "def monotonicCol(Data, colname):\n",
    "    #Takes a column that should have monotonically increasing data for a column (number of deaths)\n",
    "    #and adjusts the column to ensure this property, iterating backwards through each fips code's entries\n",
    "    ls = []\n",
    "    tempvals = []\n",
    "    for fips in Data.FIPS.unique():\n",
    "        vals = list(Data[Data['FIPS'] == fips][colname])\n",
    "        flag = True\n",
    "        for val in reversed(vals):\n",
    "            if flag:\n",
    "                flag = False\n",
    "                maxval = val\n",
    "                tempvals.append(maxval)\n",
    "            else:\n",
    "                if val > maxval:\n",
    "                    tempvals.append(maxval)\n",
    "                else:\n",
    "                    maxval = val\n",
    "                    tempvals.append(val)\n",
    "        ls.extend(reversed(tempvals))\n",
    "        tempvals = []\n",
    "    return ls\n",
    "\n",
    "def cumtoDaily(Data, colname):\n",
    "    #Takes cumulative column data and turns the data into daily changes \n",
    "    ls = []\n",
    "    column = Data[colname]\n",
    "    for fips in Data.FIPS.unique():\n",
    "        ls.extend(list(Data[Data['FIPS'] == fips][colname].diff().fillna(0)))\n",
    "    return ls\n",
    "\n",
    "def znormalize(ls):\n",
    "#normalizes a list, if std=0 return the list\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        return np.array(ls)\n",
    "    else:\n",
    "        val = (ls - np.mean(ls))/np.std(ls)\n",
    "        return (ls - np.mean(ls))/np.std(ls)\n",
    "\n",
    "def znormalize_nozeros(ls):\n",
    "#normalizes a list, if std=0 just pass\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        pass\n",
    "    else:\n",
    "        return (ls - np.mean(ls))/np.std(ls)\n",
    "    \n",
    "def noinf(arr):\n",
    "    #Removes inf from list of lists\n",
    "    newarr = []\n",
    "    for x in arr:\n",
    "        temp = x\n",
    "        temp[temp == np.inf] = 9999\n",
    "        newarr.append(x)\n",
    "    return newarr\n",
    "\n",
    "def nonzerofips(arr):\n",
    "    #Takes in dataset, returns indices of data that do not have a list with all 0's\n",
    "    ind = []\n",
    "    for i in range(len(arr)):\n",
    "        if np.std(arr[i]) != 0:\n",
    "            ind.append(i)\n",
    "    return ind\n",
    "\n",
    "def makeZ(Data):\n",
    "    #Creates DTW linkage matrix using DTAIdistance and scipy\n",
    "    distance = dtw.distance_matrix_fast(Data,compact=True)\n",
    "    Z = linkage(distance, method='complete')\n",
    "    return Z\n",
    "\n",
    "def fillnonzero(OrigData, clusters):\n",
    "    #Takes a clustering from a dataset with nonzero entries.\n",
    "    #Adds to that clustering another cluster for all 0's\n",
    "    n = 0\n",
    "    newclusters = []\n",
    "    for i in range(len(OrigData)):\n",
    "        if np.std(OrigData[i]) == 0:\n",
    "            newclusters.append(0)\n",
    "        else:\n",
    "            newclusters.append(clusters[n])\n",
    "            n += 1\n",
    "    return newclusters\n",
    "\n",
    "def makeX(Data, DTW, cluster_col, cluster_num, fipsname='FIPS', deathsname='Deaths'):\n",
    "    #Takes in the dataset, cluster column and number, and gives out the deaths info in this cluster\n",
    "    #In the form able to be processed by hmmlearn's HMM modules    \n",
    "    fips = list(DTW[DTW[cluster_col] == cluster_num]['FIPS'])\n",
    "    Rows = Data[Data[fipsname].isin(fips)]\n",
    "    RawData = makeHMMUnSupData(Rows, deathsname, fipsname)\n",
    "    temp = []\n",
    "    lengths = []\n",
    "    for i in RawData:\n",
    "        temp.extend(i)\n",
    "        lengths.append(len(i))\n",
    "    temp = np.array(temp).reshape(-1,1)\n",
    "    return [temp, lengths]\n",
    "\n",
    "def makeHMM(X):\n",
    "    #Takes in data from makeX, and uses the Elbow method to determine the optimal number of \n",
    "    #states needed in the HMM, and returns the HMM with that optimal number of states\n",
    "    scores = []\n",
    "    Flag = True\n",
    "    val = 999\n",
    "    for i in range(1,31):\n",
    "        tempmodel = hmm.GaussianHMM(n_components=i, covariance_type=\"full\")\n",
    "        #Tries to make the model fit, can fail if data not diverse enough\n",
    "        try:\n",
    "            if Flag:\n",
    "                tempmodel.fit(X[0],X[1])\n",
    "                scores.append(tempmodel.score(X[0],X[1]))\n",
    "                if i > 10:\n",
    "                    if scores[-1] > 0 and scores[-1] < scores[-2]:\n",
    "                        Flag = False\n",
    "        except:\n",
    "            val = i - 1\n",
    "            Flag = False\n",
    "    #If the data only accepts less than 4 states to work, we chose the max number of states to describe it\n",
    "    if val < 5:\n",
    "        return hmm.GaussianHMM(n_components = val, covariance_type=\"full\").fit(X[0],X[1])\n",
    "    else:\n",
    "    #We do an elbow method otherwise\n",
    "        n = 0\n",
    "        #finding number of negative entries\n",
    "        for j in scores:\n",
    "            if j < 0:\n",
    "                n += 1\n",
    "        #gettin index of best point by elbow method (using first derivative)\n",
    "        ind = np.argmax(np.diff(scores)[(n + 1):]/scores[(n + 2):])\n",
    "        return hmm.GaussianHMM(n_components = ind + n + 3, covariance_type=\"full\").fit(X[0],X[1])\n",
    "    \n",
    "def makeHMMlist(Data, DTW, cluster_col):\n",
    "    labels = np.sort(DTW[cluster_col].dropna().unique())\n",
    "    HMM_list = [0] * len(labels)\n",
    "    n = 0\n",
    "    for i in labels:\n",
    "        X = makeX(Data, DTW, cluster_col, i)\n",
    "        ls = [a.tolist()[0] for a in X[0]]\n",
    "        HMM_list[n] = makeHMM(X)\n",
    "        n += 1\n",
    "    return [HMM_list, labels]\n",
    "        \n",
    "def makeFipsPrediction(HMM, Data, fipscode, length=14, n_iters=10):\n",
    "    #Takes in an HMM, a dataset (either JHU, NYT_F, or NYT_W) and a fips code,\n",
    "    #Gives the HMM state predictions and emission predictions\n",
    "    #Does this predictions n_iters times, and reports the average states/emissions\n",
    "    X = makeHMMUnSupData(Data[Data['FIPS']==fipscode])[0]\n",
    "    states = HMM.predict(np.array(X).reshape(-1,1))\n",
    "    transmat_cdf = np.cumsum(HMM.transmat_, axis=1)\n",
    "    Emissions = [0.0] * length\n",
    "    States = [0.0] * length\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        for j in range(length):\n",
    "            random_state = check_random_state(HMM.random_state)\n",
    "            if j == 0:\n",
    "                next_state = (transmat_cdf[states[-1]] > random_state.rand()).argmax()\n",
    "            else:\n",
    "                next_state = (transmat_cdf[next_state] > random_state.rand()).argmax()\n",
    "            \n",
    "            next_obs = HMM._generate_sample_from_state(next_state, random_state)\n",
    "            \n",
    "            Emissions[j] += next_obs[0]/n_iters\n",
    "            States[j] += next_state/n_iters\n",
    "            \n",
    "    return States, Emissions      \n",
    "\n",
    "def makeHMMListPrediction(HMMList, Data, colname, DTW, length=14, n_iters=10):\n",
    "    HMMs = HMMList[0]\n",
    "    labels = HMMList[1]\n",
    "    PredictionFrame = DTW[~DTW[colname].isna()][['FIPS']]\n",
    "    \n",
    "    for i in range(length):\n",
    "        PredictionFrame[str(1 + i)] = 0\n",
    "    n = 0\n",
    "    \n",
    "    for i in labels:\n",
    "        codes = DTW[DTW[colname] == i]['FIPS'].unique().tolist()\n",
    "        HMM = HMMs[n]\n",
    "        for code in codes:\n",
    "            Prediction = makeFipsPrediction(HMM, Data, code, length, n_iters)[1]\n",
    "            for j in range(length):\n",
    "                PredictionFrame.loc[PredictionFrame['FIPS'] == code, str(j + 1)] = Prediction[j]\n",
    "        n += 1\n",
    "    return PredictionFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_iterations=10):\n",
    "    #NYT Data (NYT_W and NYT_F)\n",
    "    #Differenced Daily Death Data\n",
    "    NYT_daily = pd.read_csv(f\"{homedir}/data/us/covid/nyt_us_counties_daily.csv\")\n",
    "    NYT_daily = NYT_daily.drop(columns=['county','state']).sort_values(['fips','date']).reset_index(drop=True)\n",
    "    NYT_daily['fips'] = NYT_daily.fips.astype(int)\n",
    "    NYT_daily['date'] = pd.to_datetime(NYT_daily['date'])\n",
    "    NYT_daily['id'] = NYT_daily.fips.astype(str).str.cat(NYT_daily.date.astype(str), sep=', ')\n",
    "    FirstDay = min(NYT_daily.date.unique())\n",
    "    LastDay = max(NYT_daily.date.unique())\n",
    "\n",
    "    #Making a time-warping of NYT daily data, so each county has a value at the starting day of 2020-01-21, the second value is\n",
    "    #the date of the first reported date from NYT\n",
    "    # and then a final value at the most recent day\n",
    "    NYT_daily_Warp = NYT_daily\n",
    "    for fips in NYT_daily.fips.unique():\n",
    "        rows = NYT_daily[NYT_daily['fips'] == fips]\n",
    "        #adding in the first day values\n",
    "        if FirstDay not in rows.date.unique():\n",
    "            NYT_daily_Warp = NYT_daily_Warp.append({'fips': fips, 'date': pd.to_datetime('2020-01-21'), 'cases': 0, 'deaths' : 0, 'id' : str(fips) + ', 2020-01-21'}, ignore_index=True)\n",
    "        #making sure each entry has the final day values\n",
    "        if LastDay not in rows.date.unique():\n",
    "            NYT_daily_Warp = NYT_daily_Warp[NYT_daily_Warp['fips'] != fips]\n",
    "    NYT_daily_Warp = NYT_daily_Warp.sort_values(['fips','date']).reset_index(drop=True)\n",
    "    NYT_daily_Warp_Death = makeHMMUnSupData(NYT_daily_Warp, 'deaths', 'fips')\n",
    "\n",
    "    #This is a list of all the counties and dates\n",
    "    County_List = list(NYT_daily.fips.unique())\n",
    "    Date_List = list(NYT_daily.date.unique())\n",
    "    #This creates a base dataframe that contains all pairs of FIPS codes with the valid dates given in Air_Qual\n",
    "    CL, DL = pd.core.reshape.util.cartesian_product([County_List, Date_List])\n",
    "    BaseFrame = pd.DataFrame(dict(fips=CL, date=DL)).sort_values(['fips','date']).reset_index(drop=True)\n",
    "    BaseFrame['id'] = BaseFrame.fips.astype(str).str.cat(BaseFrame.date.astype(str), sep=', ')\n",
    "\n",
    "    #Making frame of all deaths at all dates to properly do DTW clustering\n",
    "    NYT_daily_Filled = BaseFrame.join(NYT_daily.set_index('id'), on='id', how='outer', lsuffix='',rsuffix='_x').sort_values(['fips', 'date']).drop(columns=['fips_x','date_x']).fillna(0).drop_duplicates(subset=['fips','date']).reset_index(drop=True)\n",
    "    #List of lists of daily death count for each county, starting 1/23/20, ending most recent date.\n",
    "    NYT_daily_Death_Filled = makeHMMUnSupData(NYT_daily_Filled, 'deaths', 'fips')\n",
    "    #JHU Data\n",
    "    JHU_tot = pd.read_csv(f\"{homedir}/data/us/covid/JHU_daily_US.csv\").sort_values(['FIPS','Date'])\n",
    "    FIPSlist = JHU_tot.FIPS.unique()\n",
    "    Datelist = JHU_tot.Date.unique()\n",
    "    Datepair = [Datelist[0],Datelist[-1]]\n",
    "\n",
    "    #Getting rid of unneded fips code in the list of total codes\n",
    "    for fips in FIPSlist:\n",
    "        rows = JHU_tot[JHU_tot['FIPS'] == fips]\n",
    "        datelist = rows.Date.unique()\n",
    "        datepair = [datelist[0],datelist[-1]]\n",
    "        if np.array_equal(Datepair,datepair) != True:\n",
    "            JHU_tot = JHU_tot.drop(list(JHU_tot[JHU_tot['FIPS'] == fips].index))\n",
    "    JHU_tot = JHU_tot.sort_values(['FIPS','Date']).reset_index(drop=True)\n",
    "\n",
    "    d = {'FIPS': JHU_tot['FIPS'], 'Date' : JHU_tot['Date'], 'Confirmed' : monotonicCol(JHU_tot,'Confirmed'),'Deaths' : monotonicCol(JHU_tot,'Deaths'),'Active' : monotonicCol(JHU_tot,'Active'),'Recovered' : monotonicCol(JHU_tot,'Recovered')}\n",
    "    #Monotonically increaasing transformation of JHU_tot\n",
    "    JHU_mono = pd.DataFrame(data=d)\n",
    "\n",
    "    d = {'FIPS': JHU_mono['FIPS'], 'Date' : JHU_mono['Date'], 'Confirmed' : cumtoDaily(JHU_mono,'Confirmed'),'Deaths' : cumtoDaily(JHU_mono,'Deaths'),'Active': cumtoDaily(JHU_mono,'Active'),'Recovered' : cumtoDaily(JHU_mono,'Recovered')}\n",
    "    #Daily changing data based on monotonically transformed data\n",
    "    JHU_daily = pd.DataFrame(data=d)\n",
    "    #List of lists of daily death count for each county, starting 3/23/20, ending most recent date.\n",
    "    JHU_daily_death = makeHMMUnSupData(JHU_daily, 'Deaths', 'FIPS')\n",
    "    \n",
    "    NYT_F = NYT_daily_Filled\n",
    "    NYT_W = NYT_daily_Warp\n",
    "    JHU = JHU_daily\n",
    "    \n",
    "    #Original dataset, making into list of np arrays\n",
    "    NYT_daily_Warp_Death = [np.array(x) for x in NYT_daily_Warp_Death]\n",
    "    NYT_daily_Death_Filled = [np.array(x) for x in NYT_daily_Death_Filled]\n",
    "    JHU_daily_death = [np.array(x) for x in JHU_daily_death]\n",
    "    \n",
    "    #Z normalization of our dataset\n",
    "    Series_NYT_W = [znormalize(x) for x in NYT_daily_Warp_Death]\n",
    "    Series_NYT_F = [znormalize(x) for x in NYT_daily_Death_Filled]\n",
    "    Series_JHU = [znormalize(x) for x in JHU_daily_death]\n",
    "\n",
    "    #Removal of Strictly 0 lists from our dataset, these will belong in cluster 0\n",
    "    Series_NYT_W_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Warp_Death]\n",
    "    Series_NYT_W_nozeros =  [x for x in Series_NYT_W_nozeros if x is not None]\n",
    "\n",
    "    Series_NYT_F_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Death_Filled]\n",
    "    Series_NYT_F_nozeros =  [x for x in Series_NYT_F_nozeros if x is not None]\n",
    "\n",
    "    Series_JHU_nozeros = [znormalize_nozeros(x) for x in JHU_daily_death]\n",
    "    Series_JHU_nozeros =  [x for x in Series_JHU_nozeros if x is not None]\n",
    "    \n",
    "    #We generate the many clusters needed for analysis\n",
    "    #Suffix \"O\": uses original unedited data\n",
    "    #\"Z\": uses z-normalized data, \"N\": uses z-normalized data, with all 0's entries in individual cluster\n",
    "    #\"T\": represents Tight, means a lower nubmer of clusters used\n",
    "    #\"L\": represents Loose, a higher number of clusters used\n",
    "    JHU_Cluster_Size = [2,3,6,3,6]\n",
    "\n",
    "    Z_JHU_O = makeZ(JHU_daily_death)\n",
    "    Z_JHU_Z = makeZ(Series_JHU)\n",
    "    Z_JHU_N = makeZ(Series_JHU_nozeros)\n",
    "\n",
    "    JHU_O = fcluster(Z_JHU_O, JHU_Cluster_Size[0], criterion ='maxclust')\n",
    "    JHU_Z_T = fcluster(Z_JHU_Z, JHU_Cluster_Size[1], criterion ='maxclust')\n",
    "    JHU_Z_L = fcluster(Z_JHU_Z, JHU_Cluster_Size[2], criterion ='maxclust')\n",
    "    JHU_N_T = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[3], criterion ='maxclust'))\n",
    "    JHU_N_L = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "    ClustersJHU = pd.DataFrame(data=JHU.FIPS.unique(),columns=['FIPS'])\n",
    "    ClustersJHU['JHU_Orig'] = JHU_O\n",
    "    ClustersJHU['JHU_Z_T'] = JHU_Z_T\n",
    "    ClustersJHU['JHU_Z_L'] = JHU_Z_L\n",
    "    ClustersJHU['JHU_N_T'] = JHU_N_T\n",
    "    ClustersJHU['JHU_N_L'] = JHU_N_L\n",
    "\n",
    "    NYT_F_Cluster_Size = [2,5,5]\n",
    "\n",
    "    Z_NYT_F_O = makeZ(NYT_daily_Death_Filled)\n",
    "    Z_NYT_F_Z = makeZ(Series_NYT_F)\n",
    "    Z_NYT_F_N = makeZ(Series_NYT_F_nozeros)\n",
    "\n",
    "    NYT_F_O = fcluster(Z_NYT_F_O, NYT_F_Cluster_Size[0], criterion ='maxclust')\n",
    "    NYT_F_Z = fcluster(Z_NYT_F_Z, NYT_F_Cluster_Size[1], criterion ='maxclust')\n",
    "    NYT_F_N = fillnonzero(Series_NYT_F,fcluster(Z_NYT_F_N, NYT_F_Cluster_Size[2], criterion ='maxclust'))\n",
    "\n",
    "    ClustersNYT_F = pd.DataFrame(data=NYT_F.fips.unique(),columns=['FIPS'])\n",
    "    ClustersNYT_F['NYT_F_Orig'] = NYT_F_O\n",
    "    ClustersNYT_F['NYT_F_Z'] = NYT_F_Z\n",
    "    ClustersNYT_F['NYT_F_N'] = NYT_F_N\n",
    "\n",
    "    NYT_W_Cluster_Size = [2,5,8,5,7]\n",
    "\n",
    "    Z_NYT_W_O = makeZ(NYT_daily_Warp_Death)\n",
    "    Z_NYT_W_Z = makeZ(Series_NYT_W)\n",
    "    Z_NYT_W_N = makeZ(Series_NYT_W_nozeros)\n",
    "\n",
    "    NYT_W_O = fcluster(Z_NYT_W_O, NYT_W_Cluster_Size[0], criterion ='maxclust')\n",
    "    NYT_W_Z_T = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[1], criterion ='maxclust')\n",
    "    NYT_W_Z_L = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[2], criterion ='maxclust')\n",
    "    NYT_W_N_T = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[3], criterion ='maxclust'))\n",
    "    NYT_W_N_L = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "    ClustersNYT_W = pd.DataFrame(data=NYT_W.fips.unique(),columns=['FIPS'])\n",
    "    ClustersNYT_W['NYT_W_Orig'] = NYT_W_O\n",
    "    ClustersNYT_W['NYT_W_Z_T'] = NYT_W_Z_T\n",
    "    ClustersNYT_W['NYT_W_Z_L'] = NYT_W_Z_L\n",
    "    ClustersNYT_W['NYT_W_N_T'] = NYT_W_N_T\n",
    "    ClustersNYT_W['NYT_W_N_L'] = NYT_W_N_L\n",
    "    \n",
    "    #Saving all the clusters in one dataframe\n",
    "    DTW_Clusters = ClustersJHU.join(ClustersNYT_F.set_index('FIPS'), on='FIPS', how='outer').join(ClustersNYT_W.set_index('FIPS'), on='FIPS', how='outer').sort_values('FIPS')\n",
    "    \n",
    "    JHU_Z_T_HMMs = makeHMMlist(JHU, DTW_Clusters, 'JHU_Z_T')\n",
    "    JHU_Z_L_HMMs = makeHMMlist(JHU, DTW_Clusters, 'JHU_Z_L')\n",
    "    JHU_N_T_HMMs = makeHMMlist(JHU, DTW_Clusters, 'JHU_N_T')\n",
    "    JHU_N_L_HMMs = makeHMMlist(JHU, DTW_Clusters, 'JHU_N_L')\n",
    "\n",
    "    NYT_F_Z_HMMs = makeHMMlist(NYT_F, DTW_Clusters, 'NYT_F_Z')\n",
    "    NYT_F_N_HMMs = makeHMMlist(NYT_F, DTW_Clusters, 'NYT_F_N')\n",
    "\n",
    "    NYT_W_Z_T_HMMs = makeHMMlist(NYT_W, DTW_Clusters, 'NYT_W_Z_T')\n",
    "    NYT_W_Z_L_HMMs = makeHMMlist(NYT_W, DTW_Clusters, 'NYT_W_Z_L')\n",
    "    NYT_W_N_T_HMMs = makeHMMlist(NYT_W, DTW_Clusters, 'NYT_W_N_T')\n",
    "    NYT_W_N_L_HMMs = makeHMMlist(NYT_W, DTW_Clusters, 'NYT_W_N_L')\n",
    "\n",
    "    JHU_Z_T_Pred = makeHMMListPrediction(JHU_Z_T_HMMs, JHU, 'JHU_Z_T', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    JHU_Z_L_Pred = makeHMMListPrediction(JHU_Z_L_HMMs, JHU, 'JHU_Z_L', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    JHU_N_T_Pred = makeHMMListPrediction(JHU_N_T_HMMs, JHU, 'JHU_N_T', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    JHU_N_L_Pred = makeHMMListPrediction(JHU_N_L_HMMs, JHU, 'JHU_N_L', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "\n",
    "    JHU_Z_T_Pred.to_csv('JHU_Z_T_Pred.csv')\n",
    "    JHU_Z_L_Pred.to_csv('JHU_Z_L_Pred.csv')\n",
    "    JHU_N_T_Pred.to_csv('JHU_N_T_Pred.csv')\n",
    "    JHU_N_L_Pred.to_csv('JHU_N_L_Pred.csv')\n",
    "\n",
    "    NYT_F_Z_Pred = makeHMMListPrediction(NYT_F_Z_HMMs, NYT_F, 'NYT_F_Z', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    NYT_F_N_Pred = makeHMMListPrediction(NYT_F_N_HMMs, NYT_F, 'NYT_F_N', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "\n",
    "    NYT_F_Z_Pred.to_csv('NYT_F_Z_Pred.csv')\n",
    "    NYT_F_N_Pred.to_csv('NYT_F_N_Pred.csv')\n",
    "\n",
    "    NYT_W_Z_T_Pred = makeHMMListPrediction(NYT_W_Z_T_HMMs, NYT_W, 'NYT_W_Z_T', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    NYT_W_Z_L_Pred = makeHMMListPrediction(NYT_W_Z_L_HMMs, NYT_W, 'NYT_W_Z_L', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    NYT_W_N_T_Pred = makeHMMListPrediction(NYT_W_N_T_HMMs, NYT_W, 'NYT_W_N_T', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "    NYT_W_N_L_Pred = makeHMMListPrediction(NYT_W_N_L_HMMs, NYT_W, 'NYT_W_N_L', DTW_Clusters, length=14, n_iters=num_iterations)\n",
    "\n",
    "    NYT_W_Z_T_Pred.to_csv('NYT_W_Z_T_Pred.csv')\n",
    "    NYT_W_Z_L_Pred.to_csv('NYT_W_Z_L_Pred.csv')\n",
    "    NYT_W_N_T_Pred.to_csv('NYT_W_N_T_Pred.csv')\n",
    "    NYT_W_N_L_Pred.to_csv('NYT_W_N_L_Pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
