{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from dtaidistance import dtw\n",
    "from dtaidistance import clustering\n",
    "import simplejson\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, leaves_list\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import sys\n",
    "repo = git.Repo(\"./\", search_parent_directories=True)\n",
    "homedir = repo.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NYT_daily_Warp_Death.txt') as f:\n",
    "    NYT_daily_Warp_Death = simplejson.load(f)\n",
    "with open('NYT_daily_Death_Filled.txt') as g:\n",
    "    NYT_daily_Death_Filled = simplejson.load(g)\n",
    "with open('JHU_daily_death.txt') as h:\n",
    "    JHU_daily_death = simplejson.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_F = pd.read_csv(f\"{homedir}/models/HMM_Work/NYT_daily_Filled.csv\", index_col=0)\n",
    "NYT_W = pd.read_csv(f\"{homedir}/models/HMM_Work/NYT_daily_Warp.csv\", index_col=0)\n",
    "JHU = pd.read_csv(f\"{homedir}/models/HMM_Work/JHU_daily.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def znormalize(ls):\n",
    "#normalizes a list, if std=0 return the list\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        return np.array(ls)\n",
    "    else:\n",
    "        val = (ls - np.mean(ls))/np.std(ls)\n",
    "        return (ls - np.mean(ls))/np.std(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def znormalize_nozeros(ls):\n",
    "#normalizes a list, if std=0 just pass\n",
    "    std = np.std(ls)\n",
    "    if std == 0.0:\n",
    "        pass\n",
    "    else:\n",
    "        return (ls - np.mean(ls))/np.std(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noinf(arr):\n",
    "    #Removes inf from list of lists\n",
    "    newarr = []\n",
    "    for x in arr:\n",
    "        temp = x\n",
    "        temp[temp == np.inf] = 9999\n",
    "        newarr.append(x)\n",
    "    return newarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzerofips(arr):\n",
    "    #Takes in dataset, returns indices of data that do not have a list with all 0's\n",
    "    ind = []\n",
    "    for i in range(len(arr)):\n",
    "        if np.std(arr[i]) != 0:\n",
    "            ind.append(i)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeZ(Data):\n",
    "    #Creates DTW linkage matrix using DTAIdistance and scipy\n",
    "    distance = dtw.distance_matrix_fast(Data,compact=True)\n",
    "    Z = linkage(distance, method='complete')\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillnonzero(OrigData, clusters):\n",
    "    #Takes a clustering from a dataset with nonzero entries.\n",
    "    #Adds to that clustering another cluster for all 0's\n",
    "    n = 0\n",
    "    newclusters = []\n",
    "    for i in range(len(OrigData)):\n",
    "        if np.std(OrigData[i]) == 0:\n",
    "            newclusters.append(0)\n",
    "        else:\n",
    "            newclusters.append(clusters[n])\n",
    "            n += 1\n",
    "    return newclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original dataset, making into list of np arrays\n",
    "NYT_daily_Warp_Death = [np.array(x) for x in NYT_daily_Warp_Death]\n",
    "NYT_daily_Death_Filled = [np.array(x) for x in NYT_daily_Death_Filled]\n",
    "JHU_daily_death = [np.array(x) for x in JHU_daily_death]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z normalization of our dataset\n",
    "Series_NYT_W = [znormalize(x) for x in NYT_daily_Warp_Death]\n",
    "Series_NYT_F = [znormalize(x) for x in NYT_daily_Death_Filled]\n",
    "Series_JHU = [znormalize(x) for x in JHU_daily_death]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Strictly 0 lists from our dataset, these will belong in cluster 0\n",
    "Series_NYT_W_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Warp_Death]\n",
    "Series_NYT_W_nozeros =  [x for x in Series_NYT_W_nozeros if x is not None]\n",
    "\n",
    "Series_NYT_F_nozeros = [znormalize_nozeros(x) for x in NYT_daily_Death_Filled]\n",
    "Series_NYT_F_nozeros =  [x for x in Series_NYT_F_nozeros if x is not None]\n",
    "\n",
    "Series_JHU_nozeros = [znormalize_nozeros(x) for x in JHU_daily_death]\n",
    "Series_JHU_nozeros =  [x for x in Series_JHU_nozeros if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We generate the many clusters needed for analysis\n",
    "#Suffix \"O\": uses original unedited data\n",
    "#\"Z\": uses z-normalized data, \"N\": uses z-normalized data, with all 0's entries in individual cluster\n",
    "#\"T\": represents Tight, means a lower nubmer of clusters used\n",
    "#\"L\": represents Loose, a higher number of clusters used\n",
    "JHU_Cluster_Size = [2,2,6,2,6]\n",
    "\n",
    "Z_JHU_O = makeZ(JHU_daily_death)\n",
    "Z_JHU_Z = makeZ(Series_JHU)\n",
    "Z_JHU_N = makeZ(Series_JHU_nozeros)\n",
    "\n",
    "JHU_O = fcluster(Z_JHU_O, JHU_Cluster_Size[0], criterion ='maxclust')\n",
    "JHU_Z_T = fcluster(Z_JHU_Z, JHU_Cluster_Size[1], criterion ='maxclust')\n",
    "JHU_Z_L = fcluster(Z_JHU_Z, JHU_Cluster_Size[2], criterion ='maxclust')\n",
    "JHU_N_T = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[3], criterion ='maxclust'))\n",
    "JHU_N_L = fillnonzero(Series_JHU,fcluster(Z_JHU_N, JHU_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "ClustersJHU = pd.DataFrame(data=JHU.FIPS.unique(),columns=['FIPS'])\n",
    "ClustersJHU['JHU_Orig'] = JHU_O\n",
    "ClustersJHU['JHU_Z_T'] = JHU_Z_T\n",
    "ClustersJHU['JHU_Z_L'] = JHU_Z_L\n",
    "ClustersJHU['JHU_N_T'] = JHU_N_T\n",
    "ClustersJHU['JHU_N_L'] = JHU_N_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_F_Cluster_Size = [2,2,5,2,5,9]\n",
    "\n",
    "Z_NYT_F_O = makeZ(NYT_daily_Death_Filled)\n",
    "Z_NYT_F_Z = makeZ(Series_NYT_F)\n",
    "Z_NYT_F_N = makeZ(Series_NYT_F_nozeros)\n",
    "\n",
    "NYT_F_O = fcluster(Z_NYT_F_O, NYT_F_Cluster_Size[0], criterion ='maxclust')\n",
    "NYT_F_Z_T = fcluster(Z_NYT_F_Z, NYT_F_Cluster_Size[1], criterion ='maxclust')\n",
    "NYT_F_Z_L = fcluster(Z_NYT_F_Z, NYT_F_Cluster_Size[2], criterion ='maxclust')\n",
    "NYT_F_N_T = fillnonzero(Series_NYT_F,fcluster(Z_NYT_F_N, NYT_F_Cluster_Size[3], criterion ='maxclust'))\n",
    "NYT_F_N_L = fillnonzero(Series_NYT_F,fcluster(Z_NYT_F_N, NYT_F_Cluster_Size[4], criterion ='maxclust'))\n",
    "NYT_F_N_L_L = fillnonzero(Series_NYT_F,fcluster(Z_NYT_F_N, NYT_F_Cluster_Size[5], criterion ='maxclust'))\n",
    "\n",
    "ClustersNYT_F = pd.DataFrame(data=NYT_F.fips.unique(),columns=['FIPS'])\n",
    "ClustersNYT_F['NYT_F_Orig'] = NYT_F_O\n",
    "ClustersNYT_F['NYT_F_Z_T'] = NYT_F_Z_T\n",
    "ClustersNYT_F['NYT_F_Z_L'] = NYT_F_Z_L\n",
    "ClustersNYT_F['NYT_F_N_T'] = NYT_F_N_T\n",
    "ClustersNYT_F['NYT_F_N_L'] = NYT_F_N_L\n",
    "ClustersNYT_F['NYT_F_N_L_L'] = NYT_F_N_L_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYT_W_Cluster_Size = [2,2,7,2,7]\n",
    "\n",
    "Z_NYT_W_O = makeZ(NYT_daily_Warp_Death)\n",
    "Z_NYT_W_Z = makeZ(Series_NYT_W)\n",
    "Z_NYT_W_N = makeZ(Series_NYT_W_nozeros)\n",
    "\n",
    "NYT_W_O = fcluster(Z_NYT_W_O, NYT_W_Cluster_Size[0], criterion ='maxclust')\n",
    "NYT_W_Z_T = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[1], criterion ='maxclust')\n",
    "NYT_W_Z_L = fcluster(Z_NYT_W_Z, NYT_W_Cluster_Size[2], criterion ='maxclust')\n",
    "NYT_W_N_T = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[3], criterion ='maxclust'))\n",
    "NYT_W_N_L = fillnonzero(Series_NYT_W,fcluster(Z_NYT_W_N, NYT_W_Cluster_Size[4], criterion ='maxclust'))\n",
    "\n",
    "ClustersNYT_W = pd.DataFrame(data=NYT_W.fips.unique(),columns=['FIPS'])\n",
    "ClustersNYT_W['NYT_W_Orig'] = NYT_W_O\n",
    "ClustersNYT_W['NYT_W_Z_T'] = NYT_W_Z_T\n",
    "ClustersNYT_W['NYT_W_Z_L'] = NYT_W_Z_L\n",
    "ClustersNYT_W['NYT_W_N_T'] = NYT_W_N_T\n",
    "ClustersNYT_W['NYT_W_N_L'] = NYT_W_N_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllClusters = ClustersJHU.join(ClustersNYT_F.set_index('FIPS'), on='FIPS', \\\n",
    "                               how='outer').join(ClustersNYT_W.set_index('FIPS'), on='FIPS', how='outer').sort_values('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllClusters.to_csv('DTW_Clustering.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
