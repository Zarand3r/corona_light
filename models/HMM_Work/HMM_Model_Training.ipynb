{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from HMM import unsupervised_HMM\n",
    "from HMM import supervised_HMM\n",
    "from HMM_helper import sample_sentence\n",
    "import simplejson\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import sys\n",
    "repo = git.Repo(\"./\", search_parent_directories=True)\n",
    "homedir = repo.working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHMMUnSupData(Input, colname, fipsname):\n",
    "    #Takes input dataframe, and gives out HMM format of data, a list of lists \n",
    "    #of the colname value, each list in the set represents one fips code.\n",
    "    Output = []\n",
    "    for fips in Input[fipsname].unique():\n",
    "        temp = list(Input[Input[fipsname] == fips][colname])\n",
    "        Output.append(temp)\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHMMmap(Output):\n",
    "    #Takes in output of makeHMMUnSupData and transforms data into list from 0 to D-1, where D is the number of unique\n",
    "    #values of the output\n",
    "    #Unqiue values in the input\n",
    "    UniqueVals = np.array(list(set(x for l in Output for x in l)))\n",
    "    UniqueVals = np.sort(UniqueVals)\n",
    "    HMMOutput = []\n",
    "    templs = []\n",
    "    Map = {}\n",
    "    RMap = {}\n",
    "    for x in range(len(UniqueVals)):\n",
    "        Map[int(UniqueVals[x])] = x\n",
    "        RMap[x] = int(UniqueVals[x])\n",
    "    for ls in Output:\n",
    "        for val in ls:\n",
    "            templs.append(Map[val])\n",
    "        HMMOutput.append(templs)\n",
    "        templs = []\n",
    "    return [Map,RMap,HMMOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeHMMSupData(UnSupData):\n",
    "    #Takes list of lists of time series data from makeHMMUnSupData and makes it into data with X and Y\n",
    "    X = []\n",
    "    Y = []\n",
    "    tempX = []\n",
    "    tempY = []\n",
    "    for ls in UnSupData:\n",
    "        lenls = len(ls)\n",
    "        for n in range(lenls):            \n",
    "            if n == 0:\n",
    "                tempX.append(ls[n])\n",
    "            elif n == lenls - 1:\n",
    "                tempY.append(ls[n])\n",
    "            else:\n",
    "                tempX.append(ls[n])\n",
    "                tempY.append(ls[n])\n",
    "        if len(tempX) != 0 and len(tempY) != 0:\n",
    "            X.append(tempX)\n",
    "            Y.append(tempY)\n",
    "        tempX = []\n",
    "        tempY = []   \n",
    "    return [X,Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeX(Data, DTW, cluster_col, cluster_num, fipsname, deathsname):\n",
    "    #Takes in the dataset, cluster column and number, and gives out the deaths info in this cluster\n",
    "    #In the form able to be processed by hmmlearn's HMM modules    \n",
    "    fips = list(DTW[DTW[cluster_col] == cluster_num]['FIPS'])\n",
    "    Rows = Data[Data[fipsname].isin(fips)]\n",
    "    RawData = makeHMMUnSupData(Rows, deathsname, fipsname)\n",
    "    #RawData = [a[0] for a in RawData]\n",
    "    temp = []\n",
    "    lengths = []\n",
    "    for i in RawData:\n",
    "        temp.extend(i)\n",
    "        lengths.append(len(i))\n",
    "    temp = np.array(temp).reshape(-1,1)\n",
    "    return [temp, lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes of deaths\n",
    "NYT_F = pd.read_csv(f\"{homedir}/models/HMM_Work/NYT_daily_Filled.csv\", index_col=0)\n",
    "NYT_W = pd.read_csv(f\"{homedir}/models/HMM_Work/NYT_daily_Warp.csv\", index_col=0)\n",
    "JHU = pd.read_csv(f\"{homedir}/models/HMM_Work/JHU_daily.csv\", index_col=0)\n",
    "#list of lists of deaths data\n",
    "with open('NYT_daily_Warp_Death.txt') as f:\n",
    "    NYT_daily_Warp_Death = simplejson.load(f)\n",
    "with open('NYT_daily_Death_Filled.txt') as g:\n",
    "    NYT_daily_Death_Filled = simplejson.load(g)\n",
    "with open('JHU_daily_death.txt') as h:\n",
    "    JHU_daily_death = simplejson.load(h)\n",
    "#DTW Based Clusters\n",
    "DTW_Clusters = pd.read_csv(f\"{homedir}/models/HMM_Work/DTW_Clustering.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data for the models\n",
    "test = makeX(NYT_F, DTW_Clusters, 'NYT_W_Z_L', 3, 'fips', \"deaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([140.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A bunch of models\n",
    "\n",
    "model1 = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
    "model2 = hmm.GMMHMM(n_components=4, n_mix=1, covariance_type=\"full\")\n",
    "model3 = hmm.GaussianHMM(n_components=10, covariance_type=\"full\")\n",
    "model4 = hmm.GMMHMM(n_components=10, n_mix=2, covariance_type=\"full\")\n",
    "model5 = hmm.GaussianHMM(n_components=20, covariance_type=\"full\")\n",
    "model6 = hmm.GMMHMM(n_components=15, n_mix=3, covariance_type=\"full\")\n",
    "model7 = hmm.GaussianHMM(n_components=4, covariance_type=\"full\", algorithm='map')\n",
    "model8 = hmm.GMMHMM(n_components=4, n_mix=2, covariance_type=\"full\", algorithm='map')\n",
    "model9 = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", algorithm='map')\n",
    "model10 = hmm.GMMHMM(n_components=10, n_mix=2, covariance_type=\"full\", algorithm='map')\n",
    "model11 = hmm.GaussianHMM(n_components=20, covariance_type=\"full\", algorithm='map')\n",
    "model12 = hmm.GMMHMM(n_components=15, n_mix=3, covariance_type=\"full\", algorithm='map')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianHMM(algorithm='viterbi', covariance_type='full', covars_prior=0.01,\n",
       "            covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
       "            min_covar=0.001, n_components=4, n_iter=10, params='stmc',\n",
       "            random_state=None, startprob_prior=1.0, tol=0.01,\n",
       "            transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jethi\\Anaconda3\\lib\\site-packages\\hmmlearn\\hmm.py:951: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  new_cov = new_cov_numer / new_cov_denom\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-4dc5838a86f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hmmlearn\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, lengths)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[0mcurr_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter_from_X_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m                 \u001b[0mframelogprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m                 \u001b[0mlogprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfwdlattice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframelogprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                 \u001b[0mcurr_logprob\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlogprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hmmlearn\\hmm.py\u001b[0m in \u001b[0;36m_compute_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             \u001b[0mlog_denses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_log_weighted_gaussian_densities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    857\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_denses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hmmlearn\\hmm.py\u001b[0m in \u001b[0;36m_compute_log_weighted_gaussian_densities\u001b[1;34m(self, X, i_comp)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m         return log_multivariate_normal_density(\n\u001b[1;32m--> 848\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_means\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_covs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovariance_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m         ) + log_cur_weights\n\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hmmlearn\\stats.py\u001b[0m in \u001b[0;36mlog_multivariate_normal_density\u001b[1;34m(X, means, covars, covariance_type)\u001b[0m\n\u001b[0;32m     35\u001b[0m         'full': _log_multivariate_normal_density_full}\n\u001b[0;32m     36\u001b[0m     return log_multivariate_normal_density_dict[covariance_type](\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     )\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hmmlearn\\stats.py\u001b[0m in \u001b[0;36m_log_multivariate_normal_density_full\u001b[1;34m(X, means, covars, min_covar)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mcv_chol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# The model is most probably stuck in a component with too\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[1;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \"\"\"\n\u001b[0;32m     90\u001b[0m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[1;32m---> 91\u001b[1;33m                          check_finite=check_finite)\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[1;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Common code for cholesky() and cho_factor().\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         raise ValueError(\n\u001b[1;32m--> 499\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m    500\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "model2.fit(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412659.0343084513"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
    "model3.fit(test[0],test[1])\n",
    "model3.score(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.78641935e-01, 6.18624625e-04, 9.69925956e-04, 7.66309930e-02,\n",
       "       1.05320001e-01, 1.37472145e-04, 9.37721758e-02, 3.78499830e-02,\n",
       "       5.85895509e-03, 1.99934441e-04])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.get_stationary_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86155.42524077903,\n",
       " array([[1.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "         1.89160770e-075, 2.52088085e-150, 0.00000000e+000],\n",
       "        [9.99998342e-001, 0.00000000e+000, 1.05986938e-096, ...,\n",
       "         6.70495089e-011, 2.50462540e-013, 3.81110875e-191],\n",
       "        [9.99998342e-001, 1.87126218e-207, 5.57076813e-044, ...,\n",
       "         6.70503660e-011, 2.50462938e-013, 4.81402682e-090],\n",
       "        ...,\n",
       "        [9.99998342e-001, 1.00671306e-208, 2.99697207e-045, ...,\n",
       "         6.70497865e-011, 2.50458637e-013, 2.58985899e-091],\n",
       "        [9.99998342e-001, 7.69135330e-204, 1.89435887e-040, ...,\n",
       "         6.70882422e-011, 2.53815694e-013, 1.63702638e-086],\n",
       "        [9.99995633e-001, 1.55723701e-148, 3.61358648e-034, ...,\n",
       "         4.15376246e-009, 8.51721169e-010, 1.33562472e-080]]))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score_samples(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "            covars_weight=1, init_params='stmc', means_prior=0, means_weight=0,\n",
       "            min_covar=0.001, n_components=2, n_iter=10, params='stmc',\n",
       "            random_state=None, startprob_prior=1.0, tol=0.01,\n",
       "            transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = [0.5, 1.0, -1.0, 0.42, 0.24]\n",
    "X2 = [2.4, 4.2, 0.5, -0.24]\n",
    "X = np.concatenate([X1, X2]).reshape(-1,1)\n",
    "lengths = [len(X1), len(X2)]\n",
    "hmm.GaussianHMM(n_components=2).fit(X, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5 ],\n",
       "       [ 1.  ],\n",
       "       [-1.  ],\n",
       "       [ 0.42],\n",
       "       [ 0.24],\n",
       "       [ 2.4 ],\n",
       "       [ 4.2 ],\n",
       "       [ 0.5 ],\n",
       "       [-0.24]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just a testing file so far, because our actual HMM clusterings are not available\n",
    "#Maknig basic list of list data from the direct NYT Data (no clustering we just take the whole dataset)\n",
    "DailyDeathUnSup = makeHMMUnSupData(NYT_daily, 'deaths', 'fips')\n",
    "#Making the mapping of number of deaths to HMM states\n",
    "[DailyDeathMap, DailyDeathRMap, DailyDeathUnSupHMM] = makeHMMmap(DailyDeathUnSup)\n",
    "#Making supervised X and Y datasets\n",
    "DailyDeathSup = makeHMMSupData(DailyDeathUnSupHMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HMM.HiddenMarkovModel at 0x1a01ee52c50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the superviesed testing data, and making a supervised HMM from this \n",
    "SupHMM = supervised_HMM(DailyDeathSup[0],DailyDeathSup[1])\n",
    "SupHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.355 1.145 1.213 1.019 0.806 0.712 0.668 0.776 0.617 0.632 0.631 0.596\n",
      " 0.632 0.562]\n"
     ]
    }
   ],
   "source": [
    "test = np.zeros(14)\n",
    "for j in range(1000): #This generates a sample of length 14, with a starting state of 2, \n",
    "    test += np.array(sample_sentence(SupHMM, DailyDeathMap, 14, 2))#state of 2 means 2 people died in the county yesterday\n",
    "print(test/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
